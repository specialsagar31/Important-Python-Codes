{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connection with redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip\n",
    "#!pip install psycopg2-binary\n",
    "#! pip install dask\n",
    "import sys\n",
    "import pandas as pd\n",
    "#import dask.dataframe as dd\n",
    "\n",
    "def redshift_connection(dbname, host, port, user, password):\n",
    "    \"\"\"\n",
    "    This function connects with the redshift database.\n",
    "    \"\"\"\n",
    "    import psycopg2 ## importing psycopg2\n",
    "    ## creating connection using psycopg2\n",
    "    conn = psycopg2.connect(dbname = dbname, host = host, port= port, user = user, password= password)\n",
    "    return conn ## return connection\n",
    "\n",
    "def data_execute(query, conn):\n",
    "    \"\"\"\n",
    "    This function fetch the data using provided query from the database.\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn) ## reading data from the database\n",
    "    return df \n",
    "\n",
    "## creating redshift connection with given data\n",
    "conn = redshift_connection(dbname= 'prod_database', \n",
    "                           host='private-redshift-cluster-1.cw41yiij7sku.ap-south-1.redshift.amazonaws.com', \n",
    "                           port= '5338', \n",
    "                           user= 'kishlaya', \n",
    "                           password= 'Nove@2021')x\n",
    "\n",
    "query = \"select * from schema.table\"\n",
    "df = data_execute(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For dumping csv to s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### getting the access \n",
    "import boto3\n",
    "import io\n",
    "## creating connection with s3\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY,aws_session_token=AWS_SESSION_TOKEN)\n",
    "\n",
    "## dumping data\n",
    "with io.StringIO() as csv_buffer:\n",
    "    df02.to_csv(csv_buffer, index=False)\n",
    "    response = s3_client.put_object(Bucket=\"subk-prod-raw-incremental-buckett\", Key=\"Credit/df02.csv\", Body=csv_buffer.getvalue())\n",
    "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For dumping model to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import boto3\n",
    "import joblib\n",
    "\n",
    "bucket_name = \"subk-prod-raw-incremental-buckett\"\n",
    "key = \"Credit/model1a.sav\"\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY,aws_session_token=AWS_SESSION_TOKEN)\n",
    "# WRITE\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    joblib.dump(gsearch, fp)\n",
    "    fp.seek(0)\n",
    "    s3_client.put_object(Body=fp.read(), Bucket=bucket_name, Key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For getting data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ### getting the access \n",
    "import boto3\n",
    "import io\n",
    "## creating connection with s3\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY,aws_session_token=AWS_SESSION_TOKEN)\n",
    "\n",
    "## getting data\n",
    "response = s3_client.get_object(Bucket=\"subk-prod-raw-incremental-buckett\", Key=\"Credit/RBL_FSM/los_history_02.csv\")\n",
    "import pandas as pd\n",
    "data = pd.read_csv(response['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple excel from s3 and stacking rows in a single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3', aws_access_key_id=AWS_ACCESS_KEY_ID,aws_secret_access_key=AWS_SECRET_ACCESS_KEY,aws_session_token=AWS_SESSION_TOKEN)\n",
    "bucket = s3.Bucket('bucket_name')\n",
    "prefix_objs = bucket.objects.filter(Prefix=\"path_to_req_folder\")\n",
    "prefix_df = pd.DataFrame()\n",
    "for obj in prefix_objs:\n",
    "    try:\n",
    "        key = obj.key\n",
    "        body = obj.get()['Body'].read()\n",
    "        temp = pd.read_excel(body,engine='openpyxl')#use engine='pyxlsb' for xlsb files\n",
    "        prefix_df=prefix_df.append(temp)\n",
    "        print('done for month',str(obj.key))\n",
    "    except Exception as e:\n",
    "        print(e,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from S3 using aws wrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "path=\"s3://bucket/path/filename\"\n",
    "session = boto3.session.Session(aws_access_key_id=AWS_ACCESS_KEY_ID,\\\n",
    "                         aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\\n",
    "                         aws_session_token=AWS_SESSION_TOKEN)\n",
    "df= wr.s3.read_excel(path=path,boto3_session=session,engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"subk-prod-raw-incremental-buckett\"\n",
    "folder = \"Credit/hostorical_data/rbl/F.Y. 2016-2017/FY16_17_Demand\"\n",
    "s3 = boto3.resource(\"s3\", aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                         aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "                         aws_session_token=AWS_SESSION_TOKEN)\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "files_in_s3 = [f.key.split(folder + \"/\")[1] for f in s3_bucket.objects.filter(Prefix=folder).all()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing all schemas and tables of redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select * from pg_namespace; - see all schemas\n",
    "#set search_path to '$user', 'public', 'req_schema_name'; - run this query in redshift to choose ur schema.\n",
    "#SELECT DISTINCT tablename FROM pg_table_def ## this will give all schema names+tables names...\n",
    "\n",
    "##see tables from seclected schema.\n",
    "query = \" SELECT DISTINCT tablename FROM pg_table_def WHERE schemaname = 'dw_credit'; \"\n",
    "c = data_execute(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting top 10 rows from multiple tables in a dict.\n",
    "d={}\n",
    "for i in c.tablename:\n",
    "    query = \" SELECT top 10 * FROM dw_credit.{0}\".format(i)\n",
    "    d[i]=data_execute(query, conn)\n",
    "    print('table {0} is done'.format(i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting multiple dfs to different sheets of a excel file.\n",
    "with pd.ExcelWriter(\"all_tables.xlsx\") as writer:\n",
    "    for i in d.keys():\n",
    "        d[i].to_excel(writer, sheet_name='{0}'.format(i[0:30]), index=False)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
